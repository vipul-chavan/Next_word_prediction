{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a6fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Vedant\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4f5e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If only I had more selfcontrol. I don’t have such an iron mind. I want to enjoy life too — not just suffer. These are comments I might get when people learn about my lifestyle. I’m one of those annoying people who eats lots of fruits and vegetables exercises five times a week saves a portion of their salary and writes or reads every morning before work — I have good selfcontrol. What’s more retaining this lifestyle doesn’t feel particularly difficult to me I don’t grit my teeth to avoid unhealth'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"Self.txt\", \"r\", encoding = \"utf8\")\n",
    "\n",
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    " \n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    " \n",
    "#replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','').replace(',','').replace(';','').replace('-','')  #new line, carriage return, unicode character --> replace by space\n",
    " \n",
    "#remove unnecessary spaces \n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d097e848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11779"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca774e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680\n",
      "The Length of sequences are:  1883\n",
      "Data:  [[ 31 106  14]\n",
      " [106  14 268]\n",
      " [ 14 268  15]\n",
      " [268  15   3]\n",
      " [ 15   3  14]\n",
      " [  3  14  76]\n",
      " [ 14  76  12]\n",
      " [ 76  12 151]\n",
      " [ 12 151  53]\n",
      " [151  53 269]]\n",
      "Response:  [268  15   3  14  76  12 151  53 269 270]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    " \n",
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    " \n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]\n",
    "len(sequence_data)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "sequences = []\n",
    " \n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "     \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]\n",
    "X = []\n",
    "y = []\n",
    " \n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "     \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4441eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a7e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 3, 10)             6800      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 680)               680680    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13736480 (52.40 MB)\n",
      "Trainable params: 13736480 (52.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    " #from tensorflow import keras\n",
    "#from keras.utils import plot_model\n",
    "#keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)\n",
    "\n",
    "# Gives ERROR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17ad0b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 6.2739 - accuracy: 0.0281\n",
      "Epoch 1: loss improved from inf to 6.27390, saving model to next_words.h5\n",
      "30/30 [==============================] - 8s 151ms/step - loss: 6.2739 - accuracy: 0.0281\n",
      "Epoch 2/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vedant\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 0s - loss: 5.9232 - accuracy: 0.0335\n",
      "Epoch 2: loss improved from 6.27390 to 5.92321, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 153ms/step - loss: 5.9232 - accuracy: 0.0335\n",
      "Epoch 3/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.8436 - accuracy: 0.0393\n",
      "Epoch 3: loss improved from 5.92321 to 5.84358, saving model to next_words.h5\n",
      "30/30 [==============================] - 4s 149ms/step - loss: 5.8436 - accuracy: 0.0393\n",
      "Epoch 4/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.7725 - accuracy: 0.0393\n",
      "Epoch 4: loss improved from 5.84358 to 5.77249, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 189ms/step - loss: 5.7725 - accuracy: 0.0393\n",
      "Epoch 5/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.7227 - accuracy: 0.0361\n",
      "Epoch 5: loss improved from 5.77249 to 5.72272, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 183ms/step - loss: 5.7227 - accuracy: 0.0361\n",
      "Epoch 6/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.6684 - accuracy: 0.0388\n",
      "Epoch 6: loss improved from 5.72272 to 5.66842, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 5.6684 - accuracy: 0.0388\n",
      "Epoch 7/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.5816 - accuracy: 0.0351\n",
      "Epoch 7: loss improved from 5.66842 to 5.58159, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 183ms/step - loss: 5.5816 - accuracy: 0.0351\n",
      "Epoch 8/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.4983 - accuracy: 0.0335\n",
      "Epoch 8: loss improved from 5.58159 to 5.49834, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 185ms/step - loss: 5.4983 - accuracy: 0.0335\n",
      "Epoch 9/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.4355 - accuracy: 0.0356\n",
      "Epoch 9: loss improved from 5.49834 to 5.43548, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 184ms/step - loss: 5.4355 - accuracy: 0.0356\n",
      "Epoch 10/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.3800 - accuracy: 0.0404\n",
      "Epoch 10: loss improved from 5.43548 to 5.37996, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 190ms/step - loss: 5.3800 - accuracy: 0.0404\n",
      "Epoch 11/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.3148 - accuracy: 0.0441\n",
      "Epoch 11: loss improved from 5.37996 to 5.31484, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 190ms/step - loss: 5.3148 - accuracy: 0.0441\n",
      "Epoch 12/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.2417 - accuracy: 0.0457\n",
      "Epoch 12: loss improved from 5.31484 to 5.24169, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 192ms/step - loss: 5.2417 - accuracy: 0.0457\n",
      "Epoch 13/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.1690 - accuracy: 0.0558\n",
      "Epoch 13: loss improved from 5.24169 to 5.16897, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 5.1690 - accuracy: 0.0558\n",
      "Epoch 14/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 5.0394 - accuracy: 0.0600\n",
      "Epoch 14: loss improved from 5.16897 to 5.03936, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 189ms/step - loss: 5.0394 - accuracy: 0.0600\n",
      "Epoch 15/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 4.8897 - accuracy: 0.0595\n",
      "Epoch 15: loss improved from 5.03936 to 4.88971, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 187ms/step - loss: 4.8897 - accuracy: 0.0595\n",
      "Epoch 16/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 4.7595 - accuracy: 0.0664\n",
      "Epoch 16: loss improved from 4.88971 to 4.75953, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 186ms/step - loss: 4.7595 - accuracy: 0.0664\n",
      "Epoch 17/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 4.5896 - accuracy: 0.0696\n",
      "Epoch 17: loss improved from 4.75953 to 4.58959, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 188ms/step - loss: 4.5896 - accuracy: 0.0696\n",
      "Epoch 18/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 4.3445 - accuracy: 0.0982\n",
      "Epoch 18: loss improved from 4.58959 to 4.34448, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 189ms/step - loss: 4.3445 - accuracy: 0.0982\n",
      "Epoch 19/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 4.1152 - accuracy: 0.1126\n",
      "Epoch 19: loss improved from 4.34448 to 4.11520, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 191ms/step - loss: 4.1152 - accuracy: 0.1126\n",
      "Epoch 20/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 3.8891 - accuracy: 0.1290\n",
      "Epoch 20: loss improved from 4.11520 to 3.88905, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 191ms/step - loss: 3.8891 - accuracy: 0.1290\n",
      "Epoch 21/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 3.6961 - accuracy: 0.1476\n",
      "Epoch 21: loss improved from 3.88905 to 3.69612, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 188ms/step - loss: 3.6961 - accuracy: 0.1476\n",
      "Epoch 22/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 3.4372 - accuracy: 0.1689\n",
      "Epoch 22: loss improved from 3.69612 to 3.43717, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 189ms/step - loss: 3.4372 - accuracy: 0.1689\n",
      "Epoch 23/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 3.2299 - accuracy: 0.1997\n",
      "Epoch 23: loss improved from 3.43717 to 3.22988, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 187ms/step - loss: 3.2299 - accuracy: 0.1997\n",
      "Epoch 24/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 2.9979 - accuracy: 0.2353\n",
      "Epoch 24: loss improved from 3.22988 to 2.99785, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 183ms/step - loss: 2.9979 - accuracy: 0.2353\n",
      "Epoch 25/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 2.8142 - accuracy: 0.2592\n",
      "Epoch 25: loss improved from 2.99785 to 2.81421, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 183ms/step - loss: 2.8142 - accuracy: 0.2592\n",
      "Epoch 26/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 2.6706 - accuracy: 0.2937\n",
      "Epoch 26: loss improved from 2.81421 to 2.67056, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 183ms/step - loss: 2.6706 - accuracy: 0.2937\n",
      "Epoch 27/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 2.4750 - accuracy: 0.3091\n",
      "Epoch 27: loss improved from 2.67056 to 2.47505, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 184ms/step - loss: 2.4750 - accuracy: 0.3091\n",
      "Epoch 28/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 2.2868 - accuracy: 0.3648\n",
      "Epoch 28: loss improved from 2.47505 to 2.28677, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 186ms/step - loss: 2.2868 - accuracy: 0.3648\n",
      "Epoch 29/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 2.0889 - accuracy: 0.4249\n",
      "Epoch 29: loss improved from 2.28677 to 2.08888, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 2.0889 - accuracy: 0.4249\n",
      "Epoch 30/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.9261 - accuracy: 0.4631\n",
      "Epoch 30: loss improved from 2.08888 to 1.92607, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 183ms/step - loss: 1.9261 - accuracy: 0.4631\n",
      "Epoch 31/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.8240 - accuracy: 0.4737\n",
      "Epoch 31: loss improved from 1.92607 to 1.82397, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 1.8240 - accuracy: 0.4737\n",
      "Epoch 32/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.6743 - accuracy: 0.5109\n",
      "Epoch 32: loss improved from 1.82397 to 1.67427, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 183ms/step - loss: 1.6743 - accuracy: 0.5109\n",
      "Epoch 33/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.6011 - accuracy: 0.5358\n",
      "Epoch 33: loss improved from 1.67427 to 1.60106, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 1.6011 - accuracy: 0.5358\n",
      "Epoch 34/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.3746 - accuracy: 0.5964\n",
      "Epoch 34: loss improved from 1.60106 to 1.37460, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 1.3746 - accuracy: 0.5964\n",
      "Epoch 35/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.3146 - accuracy: 0.6134\n",
      "Epoch 35: loss improved from 1.37460 to 1.31461, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 182ms/step - loss: 1.3146 - accuracy: 0.6134\n",
      "Epoch 36/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.2552 - accuracy: 0.6182\n",
      "Epoch 36: loss improved from 1.31461 to 1.25515, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 1.2552 - accuracy: 0.6182\n",
      "Epoch 37/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.1842 - accuracy: 0.6373\n",
      "Epoch 37: loss improved from 1.25515 to 1.18415, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 186ms/step - loss: 1.1842 - accuracy: 0.6373\n",
      "Epoch 38/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.0705 - accuracy: 0.6760\n",
      "Epoch 38: loss improved from 1.18415 to 1.07055, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 185ms/step - loss: 1.0705 - accuracy: 0.6760\n",
      "Epoch 39/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.9648 - accuracy: 0.6984\n",
      "Epoch 39: loss improved from 1.07055 to 0.96476, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 184ms/step - loss: 0.9648 - accuracy: 0.6984\n",
      "Epoch 40/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.8755 - accuracy: 0.7292\n",
      "Epoch 40: loss improved from 0.96476 to 0.87555, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 0.8755 - accuracy: 0.7292\n",
      "Epoch 41/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.8675 - accuracy: 0.7307\n",
      "Epoch 41: loss improved from 0.87555 to 0.86755, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 185ms/step - loss: 0.8675 - accuracy: 0.7307\n",
      "Epoch 42/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.8735 - accuracy: 0.7286\n",
      "Epoch 42: loss did not improve from 0.86755\n",
      "30/30 [==============================] - 5s 179ms/step - loss: 0.8735 - accuracy: 0.7286\n",
      "Epoch 43/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.7077 - accuracy: 0.7876\n",
      "Epoch 43: loss improved from 0.86755 to 0.70775, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 183ms/step - loss: 0.7077 - accuracy: 0.7876\n",
      "Epoch 44/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6082 - accuracy: 0.8125\n",
      "Epoch 44: loss improved from 0.70775 to 0.60823, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 182ms/step - loss: 0.6082 - accuracy: 0.8125\n",
      "Epoch 45/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.5439 - accuracy: 0.8317\n",
      "Epoch 45: loss improved from 0.60823 to 0.54390, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 184ms/step - loss: 0.5439 - accuracy: 0.8317\n",
      "Epoch 46/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.4772 - accuracy: 0.8534\n",
      "Epoch 46: loss improved from 0.54390 to 0.47723, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 183ms/step - loss: 0.4772 - accuracy: 0.8534\n",
      "Epoch 47/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.8609\n",
      "Epoch 47: loss improved from 0.47723 to 0.45775, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 183ms/step - loss: 0.4578 - accuracy: 0.8609\n",
      "Epoch 48/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.8704\n",
      "Epoch 48: loss improved from 0.45775 to 0.43070, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 182ms/step - loss: 0.4307 - accuracy: 0.8704\n",
      "Epoch 49/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3854 - accuracy: 0.8890\n",
      "Epoch 49: loss improved from 0.43070 to 0.38537, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 186ms/step - loss: 0.3854 - accuracy: 0.8890\n",
      "Epoch 50/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.8943\n",
      "Epoch 50: loss improved from 0.38537 to 0.37147, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 0.3715 - accuracy: 0.8943\n",
      "Epoch 51/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.8943\n",
      "Epoch 51: loss improved from 0.37147 to 0.34880, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 0.3488 - accuracy: 0.8943\n",
      "Epoch 52/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9193\n",
      "Epoch 52: loss improved from 0.34880 to 0.28517, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 188ms/step - loss: 0.2852 - accuracy: 0.9193\n",
      "Epoch 53/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.9219\n",
      "Epoch 53: loss improved from 0.28517 to 0.26730, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 187ms/step - loss: 0.2673 - accuracy: 0.9219\n",
      "Epoch 54/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.9278\n",
      "Epoch 54: loss improved from 0.26730 to 0.24293, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 183ms/step - loss: 0.2429 - accuracy: 0.9278\n",
      "Epoch 55/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9405\n",
      "Epoch 55: loss improved from 0.24293 to 0.23162, saving model to next_words.h5\n",
      "30/30 [==============================] - 5s 183ms/step - loss: 0.2316 - accuracy: 0.9405\n",
      "Epoch 56/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.9288\n",
      "Epoch 56: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 177ms/step - loss: 0.2391 - accuracy: 0.9288\n",
      "Epoch 57/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.9140\n",
      "Epoch 57: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 179ms/step - loss: 0.3075 - accuracy: 0.9140\n",
      "Epoch 58/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 0.8980\n",
      "Epoch 58: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 179ms/step - loss: 0.3187 - accuracy: 0.8980\n",
      "Epoch 59/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.9033\n",
      "Epoch 59: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 179ms/step - loss: 0.3238 - accuracy: 0.9033\n",
      "Epoch 60/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3124 - accuracy: 0.9055\n",
      "Epoch 60: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 178ms/step - loss: 0.3124 - accuracy: 0.9055\n",
      "Epoch 61/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3408 - accuracy: 0.8922\n",
      "Epoch 61: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 179ms/step - loss: 0.3408 - accuracy: 0.8922\n",
      "Epoch 62/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3903 - accuracy: 0.8837\n",
      "Epoch 62: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 180ms/step - loss: 0.3903 - accuracy: 0.8837\n",
      "Epoch 63/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3891 - accuracy: 0.8800\n",
      "Epoch 63: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 179ms/step - loss: 0.3891 - accuracy: 0.8800\n",
      "Epoch 64/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3705 - accuracy: 0.8911\n",
      "Epoch 64: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 177ms/step - loss: 0.3705 - accuracy: 0.8911\n",
      "Epoch 65/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3264 - accuracy: 0.8970\n",
      "Epoch 65: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 178ms/step - loss: 0.3264 - accuracy: 0.8970\n",
      "Epoch 66/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9076\n",
      "Epoch 66: loss did not improve from 0.23162\n",
      "30/30 [==============================] - 5s 177ms/step - loss: 0.2858 - accuracy: 0.9076\n",
      "Epoch 67/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9384\n",
      "Epoch 67: loss improved from 0.23162 to 0.20690, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 189ms/step - loss: 0.2069 - accuracy: 0.9384\n",
      "Epoch 68/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9416\n",
      "Epoch 68: loss improved from 0.20690 to 0.18214, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 187ms/step - loss: 0.1821 - accuracy: 0.9416\n",
      "Epoch 69/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9480\n",
      "Epoch 69: loss improved from 0.18214 to 0.16595, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 186ms/step - loss: 0.1659 - accuracy: 0.9480\n",
      "Epoch 70/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9469\n",
      "Epoch 70: loss improved from 0.16595 to 0.15428, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 0.1543 - accuracy: 0.9469\n",
      "Epoch 71/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9474\n",
      "Epoch 71: loss improved from 0.15428 to 0.14837, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 185ms/step - loss: 0.1484 - accuracy: 0.9474\n",
      "Epoch 72/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9517\n",
      "Epoch 72: loss improved from 0.14837 to 0.13812, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 185ms/step - loss: 0.1381 - accuracy: 0.9517\n",
      "Epoch 73/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9602\n",
      "Epoch 73: loss improved from 0.13812 to 0.11930, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 0.1193 - accuracy: 0.9602\n",
      "Epoch 74/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9586\n",
      "Epoch 74: loss improved from 0.11930 to 0.11566, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 184ms/step - loss: 0.1157 - accuracy: 0.9586\n",
      "Epoch 75/75\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9628\n",
      "Epoch 75: loss improved from 0.11566 to 0.10641, saving model to next_words.h5\n",
      "30/30 [==============================] - 6s 186ms/step - loss: 0.1064 - accuracy: 0.9628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2882ef24810>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=75, batch_size=64, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29d3d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'temptation-avoidance']\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "as\n",
      "['of', 'the', 'everyday']\n",
      "1/1 [==============================] - 1s 951ms/step\n",
      "temptation\n",
      "['scored', 'high', 'on']\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "trait\n",
      "['are', 'like', 'having']\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "researchers\n",
      "Execution completed.....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    " \n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    " \n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "   \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "   \n",
    "  print(predicted_word)\n",
    "  return predicted_word\n",
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "   \n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break\n",
    "   \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "         \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "           \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821e592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
